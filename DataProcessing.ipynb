{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import linregress\n",
    "from datetime import timedelta\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = [\n",
    "    #'rdatatable_datatable',\n",
    "    #'facebook_react',\n",
    "    'freecad_freecad',\n",
    "    #'angular_angular_js',\n",
    "    #\"surrealdb_surrealdb\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues = list()\n",
    "issues_comments = list()\n",
    "pull_requests = list()\n",
    "pull_request_comments = list()\n",
    "\n",
    "for file in projects:\n",
    "    temp_df = pd.read_excel(f'Files/{file}_issues.xlsx')\n",
    "    temp_df['project'] = file\n",
    "    issues.append(temp_df)\n",
    "\n",
    "    temp_df = pd.read_excel(f'Files/{file}_issues_comments.xlsx')\n",
    "    temp_df['project'] = file\n",
    "    issues_comments.append(temp_df)\n",
    "\n",
    "    temp_df = pd.read_excel(f'Files/{file}_pull_requests.xlsx')\n",
    "    temp_df['project'] = file\n",
    "    pull_requests.append(temp_df)\n",
    "\n",
    "    temp_df = pd.read_excel(f'Files/{file}_pull_request_comments.xlsx')\n",
    "    temp_df['project'] = file\n",
    "    pull_request_comments.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues = pd.concat(issues)\n",
    "df_issues_comments = pd.concat(issues_comments)\n",
    "df_pull_requests = pd.concat(pull_requests)\n",
    "df_pull_requests_comments = pd.concat(pull_request_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues['created_by'] = df_issues['created_by']\\\n",
    "    .str.replace('https://api.github.com/users/', '', regex = False)\n",
    "\n",
    "df_pull_requests['created_by'] = df_pull_requests['created_by']\\\n",
    "    .str.replace('https://api.github.com/users/', '', regex = False)\n",
    "\n",
    "df_issues_comments['created_by'] = df_issues_comments['created_by']\\\n",
    "    .str.extract(r'login=\"([^\"]+)\"')\n",
    "\n",
    "df_pull_requests_comments['created_by'] = df_pull_requests_comments['created_by']\\\n",
    "    .str.extract(r'login=\"([^\"]+)\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_users = pd.concat([\n",
    "        df_issues[['created_by', 'project']],\n",
    "        df_pull_requests[['created_by', 'project']],\n",
    "        df_issues_comments[['created_by', 'project']],\n",
    "        df_pull_requests_comments[['created_by', 'project']]\n",
    "    ])\\\n",
    "    .drop_duplicates()\\\n",
    "    .reset_index()\\\n",
    "    [['created_by', 'project']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/3ypglz8x6bx6_n9y6760l6n9h4gtsy/T/ipykernel_71130/2069917712.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pr_reviewers_by_month['created_at'] = pd.to_datetime(pr_reviewers_by_month['created_at'], errors='coerce').dt.strftime('%Y-%m')\n"
     ]
    }
   ],
   "source": [
    "pr_reviewers_by_month = df_pull_requests_comments[['created_by', 'created_at', 'pull_request_id', 'project']]\n",
    "pr_reviewers_by_month['created_at'] = pd.to_datetime(pr_reviewers_by_month['created_at'], errors='coerce').dt.strftime('%Y-%m')\n",
    "pr_reviewers_by_month = pr_reviewers_by_month.drop_duplicates()\n",
    "pr_reviewers_by_month = pr_reviewers_by_month.groupby(['created_at', 'created_by', 'project']).count().reset_index()\n",
    "pr_reviewers_by_month = pr_reviewers_by_month.rename(columns={'pull_request_id': 'number_of_revisions'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/3ypglz8x6bx6_n9y6760l6n9h4gtsy/T/ipykernel_71130/1794930000.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  issues_commented['created_at'] = pd.to_datetime(issues_commented['created_at'], errors='coerce').dt.strftime('%Y-%m')\n"
     ]
    }
   ],
   "source": [
    "issues_commented = df_issues_comments[['created_by', 'created_at', 'issue_id', 'project']]\n",
    "issues_commented['created_at'] = pd.to_datetime(issues_commented['created_at'], errors='coerce').dt.strftime('%Y-%m')\n",
    "issues_commented = issues_commented.drop_duplicates()\n",
    "issues_commented = issues_commented.groupby(['created_at', 'created_by', 'project']).count().reset_index()\n",
    "issues_commented = issues_commented.rename(columns={'issue_id': 'number_of_comments_issues'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/3ypglz8x6bx6_n9y6760l6n9h4gtsy/T/ipykernel_71130/2771644957.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  issues_created['created_at'] = pd.to_datetime(issues_created['created_at'], errors='coerce').dt.strftime('%Y-%m')\n"
     ]
    }
   ],
   "source": [
    "issues_created = df_issues[['created_by', 'created_at', 'id', 'project']]\n",
    "issues_created['created_at'] = pd.to_datetime(issues_created['created_at'], errors='coerce').dt.strftime('%Y-%m')\n",
    "issues_created = issues_created.drop_duplicates()\n",
    "issues_created = issues_created.groupby(['created_at', 'created_by', 'project']).count().reset_index()\n",
    "issues_created = issues_created.rename(columns={'id': 'number_of_issues'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c1/3ypglz8x6bx6_n9y6760l6n9h4gtsy/T/ipykernel_71130/605893098.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pull_request_created['created_at'] = pd.to_datetime(pull_request_created['created_at'], errors='coerce').dt.strftime('%Y-%m')\n"
     ]
    }
   ],
   "source": [
    "pull_request_created = df_pull_requests[['created_by', 'created_at', 'id', 'project']]\n",
    "pull_request_created['created_at'] = pd.to_datetime(pull_request_created['created_at'], errors='coerce').dt.strftime('%Y-%m')\n",
    "pull_request_created = pull_request_created.drop_duplicates()\n",
    "pull_request_created = pull_request_created.groupby(['created_at', 'created_by', 'project']).count().reset_index()\n",
    "pull_request_created = pull_request_created.rename(columns={'id': 'number_of_pr'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues_interaction = pd.concat([\n",
    "        df_issues_comments[['issue_id', 'created_by', 'created_at', 'project']],\n",
    "        df_issues[['id', 'created_by', 'created_at', 'project']].rename(columns = {'id': 'issue_id'})\n",
    "    ], ignore_index=True)\n",
    "\n",
    "df_pr_interaction = pd.concat(\n",
    "    [\n",
    "        df_pull_requests_comments[['pull_request_id', 'created_by', 'created_at', 'project']],\n",
    "        df_pull_requests[['id', 'created_by', 'created_at', 'project']].rename(columns = {'id': 'pull_request_id'})\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_issues_interaction['object'] = 'Issue'\n",
    "df_pr_interaction['object'] = 'PullRequest'\n",
    "\n",
    "df_interaction = pd.concat([\n",
    "        df_pr_interaction.rename(columns = {'pull_request_id': 'id'}),\n",
    "        df_issues_interaction.rename(columns = {'issue_id': 'id'})\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "df_interaction['created_at'] = pd.to_datetime(df_interaction['created_at']).dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interaction = df_interaction\\\n",
    "    .sort_values(by = ['object', 'created_at'])\\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "interactions = []\n",
    "interaction_points = defaultdict(int)\n",
    "\n",
    "for project in df_interaction['project'].unique():\n",
    "    temp_df = df_interaction[df_interaction['project'] == project]\n",
    "    \n",
    "    for obj in temp_df['object'].unique():\n",
    "        obj_df = temp_df[temp_df['object'] == obj]\n",
    "\n",
    "        for page in obj_df['id'].unique():\n",
    "            page_df = obj_df[obj_df['id'] == page]\n",
    "            \n",
    "            previous_users = set()\n",
    "\n",
    "            for _, row in page_df.iterrows():\n",
    "                current_user = row['created_by']\n",
    "                created_at = row['created_at']\n",
    "                \n",
    "                for user in previous_users:\n",
    "                    interaction_points[(current_user, user, created_at, project)] += 1\n",
    "                \n",
    "                previous_users.add(current_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = pd.DataFrame(\n",
    "    [(dev_a, dev_b, created_at, project, points) for (dev_a, dev_b, created_at, project), points in interaction_points.items()],\n",
    "    columns=['Developer_A', 'Interacted_With', 'Created_At', 'Project', 'Points']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = interactions_df\\\n",
    "    .groupby(['Developer_A', 'Interacted_With', 'Created_At', 'Project'])\\\n",
    "    .agg(Total_Interactions_A_to_B=('Points', 'sum'))\\\n",
    "    .reset_index()\n",
    "\n",
    "reverse_interactions = interactions_df\\\n",
    "    .rename(columns={'Developer_A': 'Interacted_With', 'Interacted_With': 'Developer_A'})\\\n",
    "    .groupby(['Developer_A', 'Interacted_With', 'Created_At', 'Project'])\\\n",
    "    .agg(Total_Interactions_B_to_A = ('Points', 'sum'))\\\n",
    "    .reset_index()\n",
    "\n",
    "df = pd.merge(\n",
    "        aggregated, \n",
    "        reverse_interactions, \n",
    "        on = ['Developer_A', 'Interacted_With', 'Created_At', 'Project'], \n",
    "        how = 'outer'\n",
    "    ).fillna(0)\n",
    "\n",
    "df['Relationship_Strength'] = df[['Total_Interactions_A_to_B', 'Total_Interactions_B_to_A']].min(axis=1)\n",
    "df = df[df['Developer_A'] != df['Interacted_With']]\n",
    "df = df.rename(columns={'Interacted_With': 'Developer_B', 'Created_At': 'Date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"monthly_metrics = []\\n\\nfor project in df['Project'].drop_duplicates():\\n    \\n    temp_df = df[df['Project'] == project]\\n    \\n    for month, month_df in temp_df.groupby(temp_df['Date']):\\n        \\n        G = nx.Graph()\\n        \\n        for _, row in month_df.iterrows():\\n            G.add_edge(\\n                row['Developer_A'], \\n                row['Developer_B'], \\n                weight=row['Relationship_Strength']\\n            )\\n        \\n        degree_centrality = nx.degree_centrality(G)\\n        betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\\n        closeness_centrality = nx.closeness_centrality(G)\\n\\n        for user in G.nodes():\\n            user_edges = list(G.edges(user, data=True))\\n            num_relationships = len(user_edges)\\n            \\n            avg_strength = (\\n                sum(edge_data['weight'] for _, _, edge_data in user_edges) / num_relationships\\n                if num_relationships > 0 else 0\\n            )\\n\\n            monthly_metrics.append({\\n                'user': user,\\n                'month': month,\\n                'degree_centrality': degree_centrality.get(user, 0),\\n                'betweenness_centrality': betweenness_centrality.get(user, 0),\\n                'closeness_centrality': closeness_centrality.get(user, 0),\\n                'num_relationships': num_relationships,\\n                'avg_strength': avg_strength,\\n                'project': project,\\n            })\\n\\n\\n    network_df = pd.DataFrame(monthly_metrics)\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''monthly_metrics = []\n",
    "\n",
    "for project in df['Project'].drop_duplicates():\n",
    "    \n",
    "    temp_df = df[df['Project'] == project]\n",
    "    \n",
    "    for month, month_df in temp_df.groupby(temp_df['Date']):\n",
    "        \n",
    "        G = nx.Graph()\n",
    "        \n",
    "        for _, row in month_df.iterrows():\n",
    "            G.add_edge(\n",
    "                row['Developer_A'], \n",
    "                row['Developer_B'], \n",
    "                weight=row['Relationship_Strength']\n",
    "            )\n",
    "        \n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "        betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "        closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "        for user in G.nodes():\n",
    "            user_edges = list(G.edges(user, data=True))\n",
    "            num_relationships = len(user_edges)\n",
    "            \n",
    "            avg_strength = (\n",
    "                sum(edge_data['weight'] for _, _, edge_data in user_edges) / num_relationships\n",
    "                if num_relationships > 0 else 0\n",
    "            )\n",
    "\n",
    "            monthly_metrics.append({\n",
    "                'user': user,\n",
    "                'month': month,\n",
    "                'degree_centrality': degree_centrality.get(user, 0),\n",
    "                'betweenness_centrality': betweenness_centrality.get(user, 0),\n",
    "                'closeness_centrality': closeness_centrality.get(user, 0),\n",
    "                'num_relationships': num_relationships,\n",
    "                'avg_strength': avg_strength,\n",
    "                'project': project,\n",
    "            })\n",
    "\n",
    "\n",
    "    network_df = pd.DataFrame(monthly_metrics)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_month(project, month, month_df):\n",
    "    G = nx.Graph()\n",
    "    for _, row in month_df.iterrows():\n",
    "        G.add_edge(\n",
    "            row['Developer_A'], \n",
    "            row['Developer_B'], \n",
    "            weight=row['Relationship_Strength']\n",
    "        )\n",
    "    \n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    \n",
    "    month_metrics = []\n",
    "    for user in G.nodes():\n",
    "        user_edges = list(G.edges(user, data=True))\n",
    "        num_relationships = len(user_edges)\n",
    "        \n",
    "        avg_strength = (\n",
    "            sum(edge_data['weight'] for _, _, edge_data in user_edges) / num_relationships\n",
    "            if num_relationships > 0 else 0\n",
    "        )\n",
    "\n",
    "        month_metrics.append({\n",
    "            'user': user,\n",
    "            'month': month,\n",
    "            'degree_centrality': degree_centrality.get(user, 0),\n",
    "            'betweenness_centrality': betweenness_centrality.get(user, 0),\n",
    "            'closeness_centrality': closeness_centrality.get(user, 0),\n",
    "            'num_relationships': num_relationships,\n",
    "            'avg_strength': avg_strength,\n",
    "            'project': project,\n",
    "        })\n",
    "    \n",
    "    return month_metrics\n",
    "\n",
    "def process_project_months(project, temp_df):\n",
    "    \"\"\"Process all months for a project in parallel.\"\"\"\n",
    "    results = Parallel(n_jobs=2)(\n",
    "        delayed(process_month)(project, month, month_df)\n",
    "        for month, month_df in temp_df.groupby(temp_df['Date'])\n",
    "    )\n",
    "    return [item for sublist in results for item in sublist]\n",
    "\n",
    "projects = df['Project'].drop_duplicates()\n",
    "results = Parallel(n_jobs=6)(\n",
    "    delayed(process_project_months)(project, df[df['Project'] == project])\n",
    "    for project in projects\n",
    ")\n",
    "\n",
    "monthly_metrics = [item for sublist in results for item in sublist]\n",
    "network_df = pd.DataFrame(monthly_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dim_users\\\n",
    "    .merge(\n",
    "        pr_reviewers_by_month,\n",
    "        on=['created_by', 'project'], \n",
    "        how='outer'\n",
    "    )\\\n",
    "    .merge(\n",
    "        issues_created,\n",
    "        on=['created_by', 'project', 'created_at'], \n",
    "        how='outer'\n",
    "    )\\\n",
    "    .merge(\n",
    "        issues_commented,\n",
    "        on=['created_by', 'project', 'created_at'], \n",
    "        how='outer'\n",
    "    )\\\n",
    "    .merge(\n",
    "        pull_request_created,\n",
    "        on=['created_by', 'project', 'created_at'], \n",
    "        how='outer'\n",
    "    )\\\n",
    "    .rename(\n",
    "        columns = {\n",
    "            'created_by': 'user',\n",
    "            'created_at': 'month'\n",
    "        }\n",
    "    )\\\n",
    "    .merge(\n",
    "        network_df,\n",
    "        on=['user', 'month', 'project']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_fill = [\n",
    "    'number_of_revisions',\n",
    "    'number_of_issues',\n",
    "    'number_of_comments_issues',\n",
    "    'number_of_pr',\n",
    "    'degree_centrality',\n",
    "    'betweenness_centrality',\n",
    "    'closeness_centrality',\n",
    "    'num_relationships',\n",
    "    'avg_strength',\n",
    "]\n",
    "\n",
    "df[columns_to_fill] = df[columns_to_fill].fillna(0)\n",
    "df['inactive_month'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "\n",
    "for (developer, project), group in df.groupby(['user', 'project']):\n",
    "    group = group.sort_values(by='month', ascending=True)\n",
    "    group['month'] = pd.to_datetime(group['month'])\n",
    "    \n",
    "    start_month = group['month'].iloc[0]\n",
    "    end_month = group['month'].iloc[-1] + pd.DateOffset(months=12)\n",
    "    \n",
    "    full_months = pd.date_range(start=start_month, end=end_month, freq='MS')\n",
    "    full_months_period = full_months.to_period('M')\n",
    "        \n",
    "    existing_months_period = group['month'].dt.to_period('M')\n",
    "    missing_months = full_months_period.difference(existing_months_period)\n",
    "    \n",
    "    if not missing_months.empty:\n",
    "        for month in missing_months:\n",
    "            all_rows.append({\n",
    "                'user': developer,\n",
    "                'project': project,\n",
    "                'month': month.start_time.strftime('%Y-%m'),\n",
    "                'number_of_revisions': 0,\n",
    "                'number_of_issues': 0,\n",
    "                'number_of_comments_issues': 0,\n",
    "                'number_of_pr': 0,\n",
    "                'degree_centrality': 0,\n",
    "                'betweenness_centrality': 0,\n",
    "                'closeness_centrality': 0,\n",
    "                'num_relationships': 0,\n",
    "                'avg_strength': 0,\n",
    "                'inactive_month': True\n",
    "            })\n",
    "\n",
    "if all_rows:\n",
    "    missing_months_df = pd.DataFrame(all_rows)\n",
    "    df = pd.concat([df, missing_months_df], ignore_index=True).sort_values(by='month', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['month'] < '2024-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"df['month_year'] = pd.to_datetime(df['month']).dt.to_period('M')\\ndf = df.sort_values(by='month', ascending=True)\\nregression_results = list()\\n\\nfor (developer, project), group in df.groupby(['user', 'project']):\\n    for idx, current_month in enumerate(group['month']):\\n        \\n        current_month = pd.to_datetime(current_month)\\n        start_month = current_month - pd.DateOffset(months=12)\\n\\n        full_months = pd.date_range(start=start_month, end=current_month, freq='MS')\\n        full_months_period = full_months.to_period('M')\\n        \\n        group_filtered = group[\\n            (group['month_year'] >= start_month.to_period('M')) & \\n            (group['month_year'] <= current_month.to_period('M'))\\n        ]\\n        missing_months = full_months_period.difference(group_filtered['month_year'])\\n        \\n        list_missing_months = list() \\n\\n        for missing_month in missing_months:\\n            missing_data = {\\n                'month': pd.to_datetime(str(missing_month)).strftime('%Y-%m'),\\n                'month_year': missing_month,\\n                'user': developer,\\n                'project': project,\\n                'inactive_month': True,\\n                'number_of_revisions': 0,\\n                'number_of_issues': 0,\\n                'number_of_comments_issues': 0,\\n                'number_of_pr': 0,\\n                'degree_centrality': 0,\\n                'betweenness_centrality': 0,\\n                'closeness_centrality': 0,\\n                'num_relationships': 0,\\n                'avg_strength': 0,\\n            }\\n            list_missing_months.append(missing_data)\\n        \\n        current_month_analysis = pd            .concat([group_filtered, pd.DataFrame(list_missing_months)], ignore_index=True)            .sort_values(by = 'month_year', ascending= False)\\n        \\n        for window in [3, 6, 9, 12]:\\n            window_data = current_month_analysis.iloc[0 : window].reset_index(drop=True)\\n            x = window_data.index\\n            \\n            for metric in [\\n                'number_of_revisions', \\n                'number_of_issues', \\n                'number_of_comments_issues', \\n                'number_of_pr', \\n                'degree_centrality', \\n                'betweenness_centrality', \\n                'closeness_centrality',\\n                'num_relationships', \\n                'avg_strength'\\n            ]:\\n                y = window_data[metric]\\n                slope, intercept, r_value, p_value, std_err = linregress(x, y)\\n                \\n                predicted_y = slope * x + intercept\\n                residuals = y - predicted_y\\n                std_dev = np.std(residuals)\\n                \\n                result = {\\n                    'user': developer,\\n                    'project': project,\\n                    'current_month': current_month,\\n                    f'{metric}_{window}_slope': slope,\\n                    f'{metric}_{window}_intercept': intercept,\\n                    f'{metric}_{window}_std_dev': std_dev\\n                }\\n                \\n                regression_results.append(result)\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df['month_year'] = pd.to_datetime(df['month']).dt.to_period('M')\n",
    "df = df.sort_values(by='month', ascending=True)\n",
    "regression_results = list()\n",
    "\n",
    "for (developer, project), group in df.groupby(['user', 'project']):\n",
    "    for idx, current_month in enumerate(group['month']):\n",
    "        \n",
    "        current_month = pd.to_datetime(current_month)\n",
    "        start_month = current_month - pd.DateOffset(months=12)\n",
    "\n",
    "        full_months = pd.date_range(start=start_month, end=current_month, freq='MS')\n",
    "        full_months_period = full_months.to_period('M')\n",
    "        \n",
    "        group_filtered = group[\n",
    "            (group['month_year'] >= start_month.to_period('M')) & \n",
    "            (group['month_year'] <= current_month.to_period('M'))\n",
    "        ]\n",
    "        missing_months = full_months_period.difference(group_filtered['month_year'])\n",
    "        \n",
    "        list_missing_months = list() \n",
    "\n",
    "        for missing_month in missing_months:\n",
    "            missing_data = {\n",
    "                'month': pd.to_datetime(str(missing_month)).strftime('%Y-%m'),\n",
    "                'month_year': missing_month,\n",
    "                'user': developer,\n",
    "                'project': project,\n",
    "                'inactive_month': True,\n",
    "                'number_of_revisions': 0,\n",
    "                'number_of_issues': 0,\n",
    "                'number_of_comments_issues': 0,\n",
    "                'number_of_pr': 0,\n",
    "                'degree_centrality': 0,\n",
    "                'betweenness_centrality': 0,\n",
    "                'closeness_centrality': 0,\n",
    "                'num_relationships': 0,\n",
    "                'avg_strength': 0,\n",
    "            }\n",
    "            list_missing_months.append(missing_data)\n",
    "        \n",
    "        current_month_analysis = pd\\\n",
    "            .concat([group_filtered, pd.DataFrame(list_missing_months)], ignore_index=True)\\\n",
    "            .sort_values(by = 'month_year', ascending= False)\n",
    "        \n",
    "        for window in [3, 6, 9, 12]:\n",
    "            window_data = current_month_analysis.iloc[0 : window].reset_index(drop=True)\n",
    "            x = window_data.index\n",
    "            \n",
    "            for metric in [\n",
    "                'number_of_revisions', \n",
    "                'number_of_issues', \n",
    "                'number_of_comments_issues', \n",
    "                'number_of_pr', \n",
    "                'degree_centrality', \n",
    "                'betweenness_centrality', \n",
    "                'closeness_centrality',\n",
    "                'num_relationships', \n",
    "                'avg_strength'\n",
    "            ]:\n",
    "                y = window_data[metric]\n",
    "                slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "                \n",
    "                predicted_y = slope * x + intercept\n",
    "                residuals = y - predicted_y\n",
    "                std_dev = np.std(residuals)\n",
    "                \n",
    "                result = {\n",
    "                    'user': developer,\n",
    "                    'project': project,\n",
    "                    'current_month': current_month,\n",
    "                    f'{metric}_{window}_slope': slope,\n",
    "                    f'{metric}_{window}_intercept': intercept,\n",
    "                    f'{metric}_{window}_std_dev': std_dev\n",
    "                }\n",
    "                \n",
    "                regression_results.append(result)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_month(developer, project, group, current_month):\n",
    "    regression_results = []\n",
    "    current_month = pd.to_datetime(current_month)\n",
    "    start_month = current_month - pd.DateOffset(months=12)\n",
    "\n",
    "    full_months = pd.date_range(start=start_month, end=current_month, freq='MS')\n",
    "    full_months_period = full_months.to_period('M')\n",
    "    \n",
    "    group_filtered = group[\n",
    "        (group['month_year'] >= start_month.to_period('M')) & \n",
    "        (group['month_year'] <= current_month.to_period('M'))\n",
    "    ]\n",
    "    missing_months = full_months_period.difference(group_filtered['month_year'])\n",
    "    \n",
    "    list_missing_months = []\n",
    "    for missing_month in missing_months:\n",
    "        missing_data = {\n",
    "            'month': pd.to_datetime(str(missing_month)).strftime('%Y-%m'),\n",
    "            'month_year': missing_month,\n",
    "            'user': developer,\n",
    "            'project': project,\n",
    "            'inactive_month': True,\n",
    "            'number_of_revisions': 0,\n",
    "            'number_of_issues': 0,\n",
    "            'number_of_comments_issues': 0,\n",
    "            'number_of_pr': 0,\n",
    "            'degree_centrality': 0,\n",
    "            'betweenness_centrality': 0,\n",
    "            'closeness_centrality': 0,\n",
    "            'num_relationships': 0,\n",
    "            'avg_strength': 0,\n",
    "        }\n",
    "        list_missing_months.append(missing_data)\n",
    "    \n",
    "    current_month_analysis = pd\\\n",
    "        .concat([group_filtered, pd.DataFrame(list_missing_months)], ignore_index=True)\\\n",
    "        .sort_values(by='month_year', ascending=False)\n",
    "    \n",
    "    for window in [3, 6, 9, 12]:\n",
    "        window_data = current_month_analysis.iloc[0:window].reset_index(drop=True)\n",
    "        x = window_data.index\n",
    "        \n",
    "        for metric in [\n",
    "            'number_of_revisions', \n",
    "            'number_of_issues', \n",
    "            'number_of_comments_issues', \n",
    "            'number_of_pr', \n",
    "            'degree_centrality', \n",
    "            'betweenness_centrality', \n",
    "            'closeness_centrality',\n",
    "            'num_relationships', \n",
    "            'avg_strength'\n",
    "        ]:\n",
    "            y = window_data[metric]\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "            \n",
    "            predicted_y = slope * x + intercept\n",
    "            residuals = y - predicted_y\n",
    "            std_dev = np.std(residuals)\n",
    "            \n",
    "            result = {\n",
    "                'user': developer,\n",
    "                'project': project,\n",
    "                'current_month': current_month,\n",
    "                f'{metric}_{window}_slope': slope,\n",
    "                f'{metric}_{window}_intercept': intercept,\n",
    "                f'{metric}_{window}_std_dev': std_dev\n",
    "            }\n",
    "            \n",
    "            regression_results.append(result)\n",
    "    return regression_results\n",
    "\n",
    "def process_group(developer, project, group):\n",
    "    group['month_year'] = pd.to_datetime(group['month']).dt.to_period('M')\n",
    "    group = group.sort_values(by='month', ascending=True)\n",
    "    \n",
    "    results = Parallel(n_jobs=2)(\n",
    "        delayed(process_month)(developer, project, group, current_month)\n",
    "        for current_month in group['month']\n",
    "    )\n",
    "    return [item for sublist in results for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby(['user', 'project'])\n",
    "\n",
    "results = Parallel(n_jobs=6)(\n",
    "    delayed(process_group)(developer, project, group)\n",
    "    for (developer, project), group in groups\n",
    ")\n",
    "\n",
    "regression_results = [result for group_results in results for result in group_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_df = pd.DataFrame(regression_results)\n",
    "\n",
    "regression_df_pivot = regression_df.pivot_table(\n",
    "    index=['user', 'project', 'current_month'],\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "regression_df_pivot.columns = [f'{col}' for col in regression_df_pivot.columns]\n",
    "regression_df_pivot.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_columns = [\n",
    "    'avg_strength_3_intercept', 'avg_strength_3_slope', 'avg_strength_3_std_dev', \n",
    "    'avg_strength_6_intercept', 'avg_strength_6_slope', 'avg_strength_6_std_dev', \n",
    "    'avg_strength_9_intercept', 'avg_strength_9_slope', 'avg_strength_9_std_dev', \n",
    "    'avg_strength_12_intercept', 'avg_strength_12_slope', 'avg_strength_12_std_dev', \n",
    "    'betweenness_centrality_3_intercept', 'betweenness_centrality_3_slope', 'betweenness_centrality_3_std_dev', \n",
    "    'betweenness_centrality_6_intercept', 'betweenness_centrality_6_slope', 'betweenness_centrality_6_std_dev', \n",
    "    'betweenness_centrality_9_intercept', 'betweenness_centrality_9_slope', 'betweenness_centrality_9_std_dev', \n",
    "    'betweenness_centrality_12_intercept', 'betweenness_centrality_12_slope', 'betweenness_centrality_12_std_dev', \n",
    "    'closeness_centrality_3_intercept', 'closeness_centrality_3_slope', 'closeness_centrality_3_std_dev', \n",
    "    'closeness_centrality_6_intercept', 'closeness_centrality_6_slope', 'closeness_centrality_6_std_dev', \n",
    "    'closeness_centrality_9_intercept', 'closeness_centrality_9_slope', 'closeness_centrality_9_std_dev', \n",
    "    'closeness_centrality_12_intercept', 'closeness_centrality_12_slope', 'closeness_centrality_12_std_dev', \n",
    "    'degree_centrality_3_intercept', 'degree_centrality_3_slope', 'degree_centrality_3_std_dev', \n",
    "    'degree_centrality_6_intercept', 'degree_centrality_6_slope', 'degree_centrality_6_std_dev', \n",
    "    'degree_centrality_9_intercept', 'degree_centrality_9_slope', 'degree_centrality_9_std_dev', \n",
    "    'degree_centrality_12_intercept', 'degree_centrality_12_slope', 'degree_centrality_12_std_dev', \n",
    "    'num_relationships_3_intercept', 'num_relationships_3_slope', 'num_relationships_3_std_dev', \n",
    "    'num_relationships_6_intercept', 'num_relationships_6_slope', 'num_relationships_6_std_dev', \n",
    "    'num_relationships_9_intercept', 'num_relationships_9_slope', 'num_relationships_9_std_dev', \n",
    "    'num_relationships_12_intercept', 'num_relationships_12_slope', 'num_relationships_12_std_dev', \n",
    "    'number_of_comments_issues_12_intercept', 'number_of_comments_issues_12_slope', 'number_of_comments_issues_12_std_dev', \n",
    "    'number_of_comments_issues_3_intercept', 'number_of_comments_issues_3_slope', 'number_of_comments_issues_3_std_dev', \n",
    "    'number_of_comments_issues_6_intercept', 'number_of_comments_issues_6_slope', 'number_of_comments_issues_6_std_dev', \n",
    "    'number_of_comments_issues_9_intercept', 'number_of_comments_issues_9_slope', 'number_of_comments_issues_9_std_dev', \n",
    "    'number_of_issues_3_intercept', 'number_of_issues_3_slope', 'number_of_issues_3_std_dev', \n",
    "    'number_of_issues_6_intercept', 'number_of_issues_6_slope', 'number_of_issues_6_std_dev', \n",
    "    'number_of_issues_9_intercept', 'number_of_issues_9_slope', 'number_of_issues_9_std_dev', \n",
    "    'number_of_issues_12_intercept', 'number_of_issues_12_slope', 'number_of_issues_12_std_dev', \n",
    "    'number_of_pr_3_intercept', 'number_of_pr_3_slope', 'number_of_pr_3_std_dev', \n",
    "    'number_of_pr_6_intercept', 'number_of_pr_6_slope', 'number_of_pr_6_std_dev', \n",
    "    'number_of_pr_9_intercept', 'number_of_pr_9_slope', 'number_of_pr_9_std_dev', \n",
    "    'number_of_pr_12_intercept', 'number_of_pr_12_slope', 'number_of_pr_12_std_dev', \n",
    "    'number_of_revisions_3_intercept', 'number_of_revisions_3_slope', 'number_of_revisions_3_std_dev', \n",
    "    'number_of_revisions_6_intercept', 'number_of_revisions_6_slope', 'number_of_revisions_6_std_dev', \n",
    "    'number_of_revisions_9_intercept', 'number_of_revisions_9_slope', 'number_of_revisions_9_std_dev',\n",
    "    'number_of_revisions_12_intercept', 'number_of_revisions_12_slope', 'number_of_revisions_12_std_dev'\n",
    "]\n",
    "\n",
    "regression_df_pivot['turnover_num'] = regression_df_pivot[activity_columns].sum(axis=1)\n",
    "regression_df_pivot['turnover'] = regression_df_pivot['turnover_num'].apply(lambda x: 'dead' if x == 0 else 'active')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_stats_list = list() \n",
    "\n",
    "for (developer, project), group in regression_df_pivot.groupby(['user', 'project']):\n",
    "\n",
    "    group = group.sort_values(by='current_month')\n",
    "    active = True \n",
    "    modified_group = []\n",
    "\n",
    "    for idx, row in group.iterrows():\n",
    "        if row['turnover'] == 'active': \n",
    "            active = True\n",
    "            modified_group.append(row) \n",
    "\n",
    "        elif row['turnover'] == 'dead' and active: \n",
    "            row['turnover'] = 'dead'\n",
    "            modified_group.append(row)\n",
    "            active = False \n",
    "\n",
    "        elif row['turnover'] == 'dead' and not active: \n",
    "            pass\n",
    "    \n",
    "    turnover_stats_list.append(modified_group)\n",
    "\n",
    "df = pd.concat([pd.DataFrame(group) for group in turnover_stats_list], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_stats_list = list()\n",
    "\n",
    "for (developer, project), group in df.groupby(['user', 'project']):\n",
    "    group = group.sort_values(by='current_month', ascending=False)\n",
    "    active = True\n",
    "    count = 0\n",
    "    modified_group = []\n",
    "\n",
    "    for idx, row in group.iterrows():\n",
    "        if row['turnover'] == 'active' and count == 0:\n",
    "            row['time_to_stop_activity'] = 12\n",
    "            modified_group.append(row)\n",
    "        \n",
    "        elif row['turnover'] == 'dead':\n",
    "            active = False \n",
    "            count = 24\n",
    "            row['turnover'] = 'dead' \n",
    "            row['time_to_stop_activity'] = 0\n",
    "            \n",
    "            modified_group.append(row)\n",
    "\n",
    "        else:\n",
    "            if count > 0:\n",
    "                count -= 1\n",
    "                if count < 12:\n",
    "                    row['turnover'] = 'pre-death'\n",
    "                    row['time_to_stop_activity'] = 0\n",
    "                elif count == 12:\n",
    "                    row['turnover'] = 'last-worked-month'\n",
    "                    row['time_to_stop_activity'] = 1\n",
    "                else:\n",
    "                    row['turnover'] = 'last-worked-year'\n",
    "                    row['time_to_stop_activity'] = count - 11\n",
    "                modified_group.append(row)\n",
    "\n",
    "    turnover_stats_list.append(modified_group)\n",
    "\n",
    "df = pd.concat([pd.DataFrame(group) for group in turnover_stats_list], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('metrics.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
